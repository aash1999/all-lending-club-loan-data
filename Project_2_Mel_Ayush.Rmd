---
output:
  html_document:
    code_folding: show
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=F, Echo =FALSE}
# Some of common RMD options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
knitr::opts_chunk$set(results="markup", warning = F, message = F)
# Can globally set option for number display format.
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
```

# 2. Logistic Regression Model Performance: How do different classification models (logistic regression and classification tree) compare in their ability to predict loan charge-offs when trained on 2013-2015 data and tested on a holdout set from the same period?

```{r, echo=TRUE}

#Load 2013-2015 Train Data
train_data <-read.csv("E:/Data science HW/Data for project 2/train.csv")

#Convert Loan Status to Numeric Values \ 1= Charged Off 0 = Fully Paid
train_data$loan_status <- ifelse(train_data$loan_status == "Charged Off", 1,0)

#Load Test Data 2013-2015
test_data <-read.csv("E:/Data science HW/Data for project 2/data_2013_2015_test.csv")   

#Convert Loan Status to Numeric Values \ 1= Charged Off 0 = Fully Paid
test_data$loan_status <- ifelse(test_data$loan_status == "Charged Off", 1,0)

#Load Test Data 2016-2018
test_data_2 <-read.csv("E:/Data science HW/Data for project 2/data_2016_2018_test.csv")

#Convert Loan Status to Numeric Values \ 1= Charged Off 0 = Fully Paid
test_data_2$loan_status <- ifelse(test_data_2$loan_status == "Charged Off", 1,0)


```

```{r, echo=TRUE}
#For Question #3
#Load Active Loans Data
active_data <-read.csv("E:/Data science HW/Data for project 2/predict.csv")

#Load 2013-2018 Train Data
all_train_data <-read.csv("E:/Data science HW/Data for project 2/train.csv")

#Convert Loan Status to Numeric Values \ 1= Charged Off 0 = Fully Paid
all_train_data$loan_status <- ifelse(all_train_data$loan_status == "Charged Off", 1,0)


```

## Train the Model

Based on the logistic regression model, all predictors included in the model are statistically significant as they are less than the 0.05 confidence interval. When looking at the coefficients, there are some noticeable insights.

Loan Amount: 0.00382: A $1 increase in the loan amount slightly increases the likelihood of a loan charged off

Interest Rate: -0.608: Higher interest rates decrease the likelihood of charge-offs

GradeG: 5.209: Loans with Grade G are more likely to be charged off compared to grade A

Employment Length less than a year: -0.2635: Borrowers with less than one year of employment are less likely to be charged off. 

```{r, echo=TRUE}

#Logistic Regression Model
log_model <-glm(loan_status ~ loan_amnt + int_rate + grade + sub_grade + dti + emp_length, data = train_data, family = binomial)

#Summary of Model
summary(log_model)

```



## Evaluate the Model Using Test Data
After evaluating the model using the test data from 2013-2015, the following can be derived:

Confusion Matrix

87,927 True Negatives: Correctly predicted loans that were fully paid.

19,868 False Negatives: Correctly predicted loans that were charged off.

502 False Positives: Loans that were charged off but incorrectly predicted as fully paid.

508 True Positives: Loans that were fully paid but incorrectly charged off.



Performance Metrics

Accuracy 81.3%: The model correctly classifies 81.3% of the loans in this test set.

Sensitivity 2.49%: The model only identifies 2.49% of actual charge-offs correctly. THIS MODEL IS POOR AT DETECTING LOAN CHARGE-OFFS.

Specificity 99.43%: The model correctly identifies 99.43% of fully paid loans.



ROC Curve & AUC

AUC 0.704: On average, the model has a 70.4% chance of distinguishing between a loan that gets charged off and one that is fully paid. Overall, this model has a moderate ability to distinguish between charged off and fully paid loans when applied to unseen data from 2013-2015. 



#libraries
library(caret)
library(pROC)

#Predict on test data
log_predict <- predict(log_model, test_data, type = "response")

#Convert Probabilities to Class 
log_predict_class <- ifelse(log_predict > 0.5, 1, 0)

#Covert Predicted and Actual to Factors with Same Levels
log_predict_class <-factor(log_predict_class, levels = c(0,1)) # Predicted Classes
test_data$loan_status <-factor(test_data$loan_status, levels = c(0,1)) # Actual Classes

#Confusion Matrix
con_matrix <- confusionMatrix(as.factor(log_predict_class), test_data$loan_status)
print(con_matrix)

#ROC Curve and AUC
roc_curve <-roc(test_data$loan_status, as.numeric(log_predict))
auc_value <- auc(roc_curve)
print(auc_value)
plot(roc_curve)





```{R}

# Get numeric columns
numeric_cols <- names(train_data)[sapply(train_data, is.numeric)]

# Set up the plotting area 
par(mfrow = c(3, 3))  # Creates a 3x3 grid of plots

# Simple histograms
for(col in numeric_cols) {
    hist(train_data[[col]], 
         main = col,
         xlab = col,
         col = "lightblue")
}

# Reset plotting parameters
par(mfrow = c(1, 1))

```


```{r}

missing_data <- function(df) {
  # Store original dimensions
  original_rows <- nrow(df)
  
  # Step 1: Remove rows with missing loan_status
  df <- df[!is.na(df$loan_status), ]
  rows_after_status <- nrow(df)
  
  # Step 2: Identify numeric and categorical columns
  numeric_cols <- sapply(df, is.numeric)
  categorical_cols <- sapply(df, function(x) is.factor(x) | is.character(x))
  
  # Step 3: Median imputation for numeric columns
  for(col in names(df)[numeric_cols]) {
    if(any(is.na(df[[col]]))) {
      col_median <- median(df[[col]], na.rm = TRUE)
      df[[col]][is.na(df[[col]])] <- col_median
      print(paste("Imputed", sum(is.na(df[[col]])), "missing values in", col, "with median:", round(col_median, 2)))
    }
  }
  
  # Step 4: Mode imputation for categorical columns
  for(col in names(df)[categorical_cols]) {
    if(any(is.na(df[[col]]))) {
      # Calculate mode (most frequent value)
      mode_val <- names(sort(table(df[[col]]), decreasing = TRUE))[1]
      df[[col]][is.na(df[[col]])] <- mode_val
      print(paste("Imputed", sum(is.na(df[[col]])), "missing values in", col, "with mode:", mode_val))
    }
  }
  
  # Print summary of cleaning
  print("\nCleaning Summary:")
  print(paste("Original number of rows:", original_rows))
  print(paste("Rows removed due to missing loan_status:", original_rows - rows_after_status))
  print(paste("Final number of rows:", nrow(df)))
  
  return(df)
}

create_features <- function(df, poly_degree) {
    # Store original dataframe
    result_df <- df
    
    # Get numeric columns except target (loan_status)
    numeric_cols <- names(df)[sapply(df, is.numeric)]
    numeric_cols <- numeric_cols[numeric_cols != "loan_status"]
    
    # 1. Create interaction terms
    cat("Creating interaction terms...\n")
    if(length(numeric_cols) >= 2) {  # Need at least 2 columns for interactions
        # Get all possible pairs of columns
        pairs <- combn(numeric_cols, 2)
        
        # Create interaction terms
        for(i in 1:ncol(pairs)) {
            col1 <- pairs[1,i]
            col2 <- pairs[2,i]
            new_col_name <- paste0("interaction_", col1, "_", col2)
            result_df[[new_col_name]] <- df[[col1]] * df[[col2]]
        }
    }
    
    # 2. Create polynomial terms
    cat("Creating polynomial terms...\n")
    for(col in numeric_cols) {
        for(degree in 2:poly_degree) {  # Start from degree 2 since degree 1 is original
            new_col_name <- paste0("poly_", col, "_degree_", degree)
            result_df[[new_col_name]] <- df[[col]]^degree
        }
    }
    
    # Print summary of new features
    n_interactions <- ncol(result_df) - ncol(df)
    cat("\nFeature Engineering Summary:\n")
    cat("Original features:", length(numeric_cols), "\n")
    cat("New features created:", n_interactions, "\n")
    cat("Total features:", ncol(result_df), "\n")
    
    return(result_df)
}

train_data <- missing_data(train_data)
test_data <- missing_data(test_data)
active_data <- missing_data(active_data)




# Example usage:
train_data <- create_features(train_data, poly_degree = 2)
test_data <- create_features(test_data, poly_degree = 2)
active_data <- create_features(active_data, poly_degree = 2)

```



```{R}
# Load required packages
library(caret)
library(pROC)
library(glmnet)

# 1. Data Preparation and Cleaning
# First, handle missing values
numeric_cols <- sapply(train_data, is.numeric)
for(col in names(train_data)[numeric_cols]) {
    train_data[[col]][is.na(train_data[[col]])] <- median(train_data[[col]], na.rm = TRUE)
    test_data[[col]][is.na(test_data[[col]])] <- median(train_data[[col]], na.rm = TRUE)
}

# Fix the loan_status encoding
# First, ensure it's numeric
train_data$loan_status <- as.numeric(as.character(train_data$loan_status))
test_data$loan_status <- as.numeric(as.character(test_data$loan_status))

# Then convert to factor with proper levels
train_data$loan_status <- factor(train_data$loan_status, 
                                levels = c(0, 1), 
                                labels = c("Fully Paid", "Charged-Off"))

test_data$loan_status <- factor(test_data$loan_status, 
                                levels = c(0, 1), 
                                labels = c("Fully Paid", "Charged-Off"))


# For training data
if("issue_d" %in% colnames(train_data)) {
    train_data <- subset(train_data, select = -c(issue_d))
    cat("issue_d column removed from training data\n")
} else {
    cat("issue_d column not found in training data\n")
}

# For test data
if("issue_d" %in% colnames(test_data)) {
    test_data <- subset(test_data, select = -c(issue_d))
    cat("issue_d column removed from test data\n")
} else {
    cat("issue_d column not found in test data\n")
}

```


```{r}


# 2. Set up cross-validation
ctrl <- trainControl(
    method = "cv",
    number = 5,
    classProbs = TRUE,
    summaryFunction = twoClassSummary,
    sampling = "down",
    verboseIter = TRUE
)

# 3. Grid for hyperparameter tuning
grid <- expand.grid(
    alpha = c(0, 0.5, 1),
    lambda = seq(0.001, 0.1, length.out = 5)
)
levels(train_data$loan_status) <- make.names(levels(train_data$loan_status))



# 4. Train improved model
set.seed(123)
improved_model <- train(
    loan_status ~ .,
    data = train_data,
    method = "glmnet",
    metric = "ROC",
    trControl = ctrl,
    tuneGrid = grid,
    family = "binomial"
)

```


```{r}
# 5. Make predictions
improved_predict <- predict(improved_model, test_data, type = "prob")[,"Charged.Off"]

# Find optimal threshold
roc_obj <- roc(test_data$loan_status, improved_predict)
optimal_coords <- coords(roc_obj, "best", ret = "threshold")
optimal_threshold <- optimal_coords$threshold

# Create class predictions using optimal threshold
improved_predict_class <- factor(
    ifelse(improved_predict > optimal_threshold, "Charged.Off", "Fully.Paid"),
    levels = c("Charged.Off", "Fully.Paid")
)

# 6. Model Evaluation
# Confusion Matrix
levels(test_data$loan_status) <- make.names(levels(test_data$loan_status))
improved_cm <- confusionMatrix(improved_predict_class, test_data$loan_status)
print(improved_cm)

# ROC and AUC
improved_roc <- roc(test_data$loan_status, improved_predict)
improved_auc <- auc(improved_roc)
print(paste("AUC:", round(improved_auc, 4)))

# 7. Visualizations
# ROC curve
plot(improved_roc, main = "ROC Curve for Improved Model")

# Feature importance
importance <- varImp(improved_model)
plot(importance, top = 10, main = "Top 10 Most Important Features")

# 8. Print detailed metrics
cat("\nDetailed Performance Metrics:\n")
cat("Accuracy:", round(improved_cm$overall['Accuracy'], 4), "\n")
cat("Sensitivity:", round(improved_cm$byClass['Sensitivity'], 4), "\n")
cat("Specificity:", round(improved_cm$byClass['Specificity'], 4), "\n")
cat("Precision:", round(improved_cm$byClass['Pos Pred Value'], 4), "\n")
cat("F1 Score:", round(2 * (improved_cm$byClass['Sensitivity'] * improved_cm$byClass['Pos Pred Value']) / 
    (improved_cm$byClass['Se+nsitivity'] + improved_cm$byClass['Pos Pred Value']), 4), "\n")

```

# Evaluating the Model Using Test Data from 2015-2018


After evaluating the model using test data from 2015-2018, the following can be derived:

Confusion Matrix

60,134 True Negatives: Correctly predicted loans that were fully paid.

17,395 False Negatives: Correctly predicted loans that were charged off.

200 False Positives: Loans that were charged off but incorrectly predicted as fully paid.

197 True Positives: Loans that were fully paid but incorrectly charged off. 


Performance Metrics

Accuracy 77.4%: The model correctly classifies 77.4% of the loans in this test set.

Sensitivity 1.12%: The model only identifies 1.12% of actual charge-offs correctly. THIS MODEL IS POOR AT DETECTING LOAN CHARGE-OFFS.

Specificity 99.67%: The model correctly identifies 99.67% of fully paid loans.



ROC Curve & AUC
AUC 0.694: On average, the model has a 69.4% chance of distinguishing between a loan that gets charged off and one that is fully paid. Overall, this model has a moderate ability to distinguish between fully paid loans when applied to unseen data from 2016-2018 but could use improvement in predicting charged off loans.  

```{r, echo=TRUE}

#libraries
library(caret)
library(pROC)

#Predict on test data
log_predict_2 <- predict(log_model, test_data_2, type = "response")

#Convert Probabilities to Class 
log_predict_class_2 <- ifelse(log_predict_2 > 0.5, 1, 0)

#Covert Predicted and Actual to Factors with Same Levels
log_predict_class_2 <-factor(log_predict_class_2, levels = c(0,1)) # Predicted Classes
test_data_2$loan_status <-factor(test_data_2$loan_status, levels = c(0,1)) # Actual Classes

#Confusion Matrix
con_matrix_2 <- confusionMatrix(as.factor(log_predict_class_2), test_data_2$loan_status)
print(con_matrix_2)

#ROC Curve and AUC
roc_curve_2 <-roc(test_data_2$loan_status, as.numeric(log_predict_2))
auc_value_2 <- auc(roc_curve_2)
print(auc_value_2)
plot(roc_curve_2)

```
```{r}
# Examine the structure and missing values
str(test_data_2)
colSums(is.na(test_data_2))

# Clean the data
test_data_2_clean <- na.omit(test_data_2)

# Check if loan_status needs recoding (if it's 1,2 instead of 0,1)
unique(test_data_2_clean$loan_status)

```
```{r}
# Load required libraries
library(caret)
library(pROC)
library(glmnet)
library(recipes)

# Keep track of loan_status levels
print("Initial loan_status levels:")
print(levels(test_data_2$loan_status))

# Handle missing values
test_data_2$dti[is.na(test_data_2$dti)] <- median(test_data_2$dti, na.rm = TRUE)
test_data_2$open_acc_6m[is.na(test_data_2$open_acc_6m)] <- median(test_data_2$open_acc_6m, na.rm = TRUE)

# Remove rows with any remaining NA values
test_data_2 <- na.omit(test_data_2)

# Create recipe for feature engineering
recipe_obj <- recipe(loan_status ~ ., data = test_data_2) %>%
  step_rm(issue_d) %>%
  step_interact(terms = ~ loan_amnt:int_rate + loan_amnt:dti + int_rate:dti) %>%
  step_poly(loan_amnt, int_rate, dti, degree = 2) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors())

# Prepare the data
prepared_data <- prep(recipe_obj) %>%
  bake(new_data = NULL)

# Set up cross-validation
ctrl <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = TRUE,
  verboseIter = TRUE
)

# Create parameter grid
grid <- expand.grid(
  alpha = seq(0, 1, by = 0.5),
  lambda = 10^seq(-4, -1, length.out = 5)
)

# Split data into predictors and response
x_matrix <- as.matrix(prepared_data %>% select(-loan_status))
y_vector <- prepared_data$loan_status
y_vector <- factor(y_vector, levels = c(0, 1), labels = c("Fully.Paid", "charged.Off"))

# Train model
set.seed(123)
tuned_model <- train(
  x = x_matrix,
  y = y_vector,
  method = "glmnet",
  trControl = ctrl,
  tuneGrid = grid,
  metric = "ROC"
)

# Print best parameters
print("Best Tuning Parameters:")
print(tuned_model$bestTune)

```


```{r}
# Make predictions
predictions_prob <- predict( tuned_model, newdata = x_matrix, type = "prob")
predictions_class <- predict(tuned_model, newdata = x_matrix)

# Verify predictions structure
print("Structure of predictions:")
print(str(predictions_class))
print(str(y_vector))
 
# Generate confusion matrix
conf_matrix <- confusionMatrix(predictions_class, y_vector)
print("Confusion Matrix and Performance Metrics:")
print(conf_matrix)

# Calculate ROC and AUC
roc_obj <- roc(y_vector, predictions_prob[,"charged.Off"])
auc_value <- auc(roc_obj)
print(paste("AUC:", round(auc_value, 3)))

# Plot ROC curve
plot(roc_obj, main = "ROC Curve")

# Print detailed metrics
metrics <- data.frame(
  Metric = c("Accuracy", "Sensitivity", "Specificity", "Precision", "AUC"),
  Value = c(
    conf_matrix$overall["Accuracy"],
    conf_matrix$byClass["Sensitivity"],
    conf_matrix$byClass["Specificity"],
    conf_matrix$byClass["Pos Pred Value"],
    auc_value
  )
)
print("Detailed Performance Metrics:")
print(metrics)

# Variable importance
importance <- varImp(tuned_model)
print("Variable Importance:")
print(importance)



```


# 3. How accurately can we predict loan charge-offs for Lending Club loans issued between 2015-2018 that are still active and might charge-off in future, using our 2013-2018 trained models from Question #2?

After using 2013-2018 data to train the logistic model, we can then use this model to predict whether active loans from 2015-2018 will charge off. After predicting probabilities and using a probability threshold of 0.5, the model predicts that there's a higher than 50% risk of 3,485 active loans being charged off. On the other hand, the model predicts that there's a 50% or lower risk of 858,829 active loans being charged off

```{r, echo=TRUE}
#library
library(dplyr)
#Train Logistic Regression Model using 2013-2018 trained data
active_model <-glm(loan_status ~ loan_amnt + int_rate + grade + sub_grade + dti + emp_length, data = all_train_data, family = binomial)
summary(active_model)

#Predict Probabilities
active_data$predicted_prob <-predict(active_model, newdata = active_data, type = "response")

#Classify loans based on probability threshold
active_data <-active_data %>%
  mutate(predicted_risk = ifelse(predicted_prob > 0.5, "High Risk", "Low Risk"))

#Summary of Predicted Charge-Offs
table(active_data$predicted_risk)
```

