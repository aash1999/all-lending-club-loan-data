---
title: "Understanding Dataset"
author: "Singh Sivaram, Aakash"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, comment = NA)
```

```{r init, include=F}
# include all your packages here
library(ezids)
library(data.table)
source("./HelperFunctions/fetchSubset.R")
```
```{r}
# all file paths
df_file_path = "../DataSet/filtered_accepted_2013_to_2018Q4.csv"

```

# Inspecting Dataset

```{r}
# Load Dataset
df_100 <- fetch_subset(df_file_path, col_names = c(-1), nrows = c(1, 100), chunk_size = 1e5)

```

## List of Column names 

```{r}
col_names <- names(df_100)
print(sort(col_names))
```
## \# of Observations

```{r}
nrows <- nrow(fread(df_file_path, select = 1, header = TRUE))
print(nrows)

```

## variables

```{r}
columns_to_target = c("annual_inc", "dti", "fico_range_high", "fico_range_low", "loan_amnt", 
  "int_rate", "earliest_cr_line", "revol_util", "delinq_2yrs", "pub_rec", 
  "total_acc", "open_acc", "installment", "home_ownership", 
  "verification_status", "delinq_amnt", "collections_12_mths_ex_med", 
  "chargeoff_within_12_mths", "mths_since_last_delinq", "purpose", "sub_grade", "issue_d", "addr_state","inq_last_12m","loan_status")


df <- fetch_subset(df_location = df_file_path, col_names = columns_to_target, nrows = c(1,-1))

```

```{r}
str(df)
```

```{r}
convert_to_factors <- function(df) {
  # Columns to convert to factors based on the provided list
  factor_cols <- c("home_ownership", "verification_status", 
                   "loan_status", "purpose", "addr_state","sub_grade")
  
  df[] <- lapply(names(df), function(col) {
    if (col %in% factor_cols) {
      return(as.factor(df[[col]]))
    } else if (is.character(df[[col]]) && all(!is.na(as.numeric(df[[col]][-1])))) {
      return(as.numeric(df[[col]]))
    } else {
      return(df[[col]])
    }
  })
  
  return(df)
}
df <- df[-1,]
df <- convert_to_factors(df)
str(df)
```


```{r}
# Convert specified columns to numeric

df$revol_util <- as.numeric(df$revol_util)
df$mths_since_last_delinq <- as.numeric(df$mths_since_last_delinq)
df$dti <- as.numeric(df$dti)
df$inq_last_12m <- as.numeric(df$inq_last_12m)


# Optional: Check for any warnings about NAs being introduced during coercion
if (any(is.na(df$revol_util))) {
  warning("NAs introduced in 'revol_util' during conversion.")
}
if (any(is.na(df$mths_since_last_delinq))) {
  warning("NAs introduced in 'mths_since_last_delinq' during conversion.")
}
if (any(is.na(df$dti))) {
  warning("NAs introduced in 'dti' during conversion.")
}

```
```{r}
str(df)
```

```{r}
# Count the number of NA values in each column
na_counts <- sapply(df, function(x) sum(is.na(x)))

# Print the counts of NA values
print(na_counts)

```

```{r}
# Load necessary libraries
library(corrplot)
setDT(df)  # Converts df to a data.table, if it isn't one already

# Select only numeric columns from the data table
numeric_df <- df[, .SD, .SDcols = sapply(df, is.numeric)]

# Calculate the correlation matrix
cor_matrix <- cor(numeric_df, use = "pairwise.complete.obs")

# Set plotting parameters for larger plot size
#par(cex.axis = 3, cex.lab = 3, cex.main = 3)  # Increase text size

# Create a larger plot window
options(repr.plot.width = 35, repr.plot.height = 35)  # Use this in RMarkdown or Jupyter Notebooks

# Plot the correlation matrix using corrplot
#png(filename = "mycorrplot.png", width = 1200, height = 800)
# Set plotting parameters for larger text size
corrplot(cor_matrix,
         type = "upper", 
         method = "circle", 
         addCoef.col = 0.2,  # Color for coefficients
         number.cex = 0.1,  # Size of the correlation coefficients
         tl.cex = 0.6,      # Size of the text labels for variables
         tl.col = "black",  # Color of the text labels
         cl.cex = 0.5,      # Size of the color legend text
         main = "Correlation Matrix",  # Title of the plot
         main.cex = 0.1)      # Size of the title text

# Close the device
#dev.off()
```

```{r}
# Load necessary libraries
library(lubridate)
library(dplyr)
library(ggplot2)

# Create a copy of the dataframe
df1 <- df

# Step 1: Ensure 'issue_d' is in Date format
df1$issue_d <- as.Date(df1$issue_d, format = "%Y-%m-%d")  # Adjust the format if necessary

# Step 2: Extract year and quarter from 'issue_d'
df1 <- df1 %>%
  mutate(year = year(issue_d), 
         quarter = quarter(issue_d))

# Step 3: Filter out rows with NA values in year or quarter
df1 <- df1 %>%
  filter(!is.na(year), !is.na(quarter))

# Step 4: Calculate the total number of loans per quarter
df_total_per_quarter <- df1 %>%
  group_by(year, quarter) %>%
  summarise(total_loans = n(), .groups = 'drop')  # Adding .groups = 'drop' to avoid warnings

# Step 5: Filter for 'Charged Off' loan status
df_charged_off <- df1 %>%
  filter(loan_status == "Charged Off")

# Step 6: Aggregate 'Charged Off' counts by year and quarter
df_charged_off_aggregated <- df_charged_off %>%
  group_by(year, quarter) %>%
  summarise(charged_off_count = n(), .groups = 'drop')  # Adding .groups = 'drop' to avoid warnings

# Step 7: Merge the total loan counts with the 'Charged Off' counts
df_merged <- df_charged_off_aggregated %>%
  left_join(df_total_per_quarter, by = c("year", "quarter")) %>%
  mutate(percentage = (charged_off_count / total_loans) * 100)

# Step 8: Create a Year-Quarter variable in the correct format and order
df_merged <- df_merged %>%
  mutate(Year_Quarter = paste0(year, ".", quarter)) %>%
  arrange(year, quarter)  # Arrange the data to ensure it's in the correct order

# Step 9: Convert Year_Quarter into a factor with levels in the correct chronological order
df_merged$Year_Quarter <- factor(df_merged$Year_Quarter, 
                                 levels = df_merged$Year_Quarter)

# Step 10: Create the line plot with percentage and display percentage as text on each point
ggplot(df_merged, aes(x = Year_Quarter, y = percentage, group = 1)) +
  geom_line(color = "red", size = 1) +
  geom_point(color = "red", size = 2) +
  geom_text(aes(label = sprintf("%.1f%%", percentage)), vjust = -1, size = 3.5) +  # Display percentage above points
  labs(x = "Year-Quarter", y = "Charged Off Percentage", title = "Percentage of Charged Off Loans by Year and Quarter") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
```{r}
# Load necessary libraries
library(dplyr)
library(lubridate)

# Ensure you're using the tidyverse for complete functionality
# Uncomment the following line if you want to load the entire tidyverse
# library(tidyverse)

# Step 1: Verify and convert 'issue_d' to Date if necessary
df$issue_d <- as.Date(df$issue_d, format = "%Y-%m-%d")  # Adjust the format if necessary

# Step 2: Filter the dataframe for 'Charged Off' loan_status and remove NaN values
charged_off_df <- df %>%
  filter(loan_status == "Charged Off") %>%
  filter(!is.na(issue_d))  # Ensure we have dates to work with

# Step 3: Create a new column for quarters if not already created
charged_off_df <- charged_off_df %>%
  mutate(quarter = quarter(issue_d), 
         year = year(issue_d),
         quarter_year = as.integer(format(issue_d, "%Y")) * 10 + ceiling(as.integer(format(issue_d, "%m")) / 3))

# Step 4: Check unique quarters available in the dataset
unique_quarters <- unique(charged_off_df$quarter_year)
cat("Unique quarters in the dataset:", unique_quarters, "\n")

# Step 5: Split the data into two groups: before and including Q2 2015, and after Q2 2015
before_2015_Q2 <- charged_off_df %>%
  filter(year < 2015 | (year == 2015 & quarter <= 2))

after_2015_Q2 <- charged_off_df %>%
  filter(year > 2015 | (year == 2015 & quarter > 2))

# Debug: Print sizes of both groups
cat("Number of records before and including Q2 2015:", nrow(before_2015_Q2), "\n")
cat("Number of records after Q2 2015:", nrow(after_2015_Q2), "\n")

# Proceed only if there are records in both groups
if (nrow(before_2015_Q2) > 0 && nrow(after_2015_Q2) > 0) {
  # Step 6: Identify numerical features in the filtered dataframe
  numerical_cols <- before_2015_Q2 %>%
    select_if(is.numeric) %>%
    colnames()

  # Step 7: Perform T-test for each numeric column, excluding those with zero observations in either group
  alpha <- 0.05  # Significance level
  t_test_results <- lapply(numerical_cols, function(col) {
    # Extract the two groups
    group1 <- before_2015_Q2[[col]]
    group2 <- after_2015_Q2[[col]]

    # Check if both groups have enough non-NA observations
    if (length(na.omit(group1)) > 0 && length(na.omit(group2)) > 0) {
      # Debug: Print intermediate values
      cat("\nColumn:", col, "\n")
      cat("N1:", length(na.omit(group1)), "N2:", length(na.omit(group2)), "\n")

      # Perform the t-test
      t_test_result <- t.test(group1, group2, var.equal = FALSE)  # Welch's t-test

      # Determine hypothesis status
      hypothesis_status <- ifelse(t_test_result$p.value <= alpha, "Reject H0", "Fail to Reject H0")

      return(data.frame(Feature = col, T_value = t_test_result$statistic, P_value = t_test_result$p.value, Hypothesis_Status = hypothesis_status))
    } else {
      cat("Skipping column:", col, "due to insufficient data in one of the groups.\n")
      return(NULL)  # Return NULL for insufficient data
    }
  })

  # Step 8: Remove NULL results and combine results into a dataframe
  t_test_results_df <- do.call(rbind, Filter(Negate(is.null), t_test_results))
  
  # Step 9: Remove unwanted features
  t_test_results_filtered <- t_test_results_df %>%
    filter(!Feature %in% c("quarter", "year", "quarter_year"))
  
  # Step 10: Print the filtered results
  print(t_test_results_filtered)

} else {
  cat("Not enough records in one of the groups to perform t-tests.\n")
}



```

```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(maps)
library(ggmap)

# Load the state coordinates
state_coords <- read.csv("../DataSet/states.csv")

# Step 1: Verify and convert 'issue_d' to Date if necessary
df$issue_d <- as.Date(df$issue_d, format = "%Y-%m-%d")  # Adjust the format if necessary

# Step 2: Filter the dataframe and remove NaN values
charged_off_df <- df %>%
  filter(!is.na(issue_d), !is.na(addr_state))  # Ensure we have dates and states to work with

# Step 3: Create a new column for quarters
charged_off_df <- charged_off_df %>%
  mutate(quarter_year = as.integer(format(issue_d, "%Y")) * 10 + ceiling(as.integer(format(issue_d, "%m")) / 3))

# Step 4: Split the data into two groups: before and including Q2 2015, and after Q2 2015
before_2015_Q2 <- charged_off_df %>%
  filter(quarter_year <= 20152)

after_2015_Q2 <- charged_off_df %>%
  filter(quarter_year > 20152)

# Step 5: Calculate total loans and charged off loans by state for both periods
total_loans_before <- before_2015_Q2 %>%
  group_by(addr_state) %>%
  summarise(total_loans = n(), .groups = "drop")

charged_off_loans_before <- before_2015_Q2 %>%
  filter(loan_status == "Charged Off") %>%
  group_by(addr_state) %>%
  summarise(charged_off_loans = n(), .groups = "drop")

total_loans_after <- after_2015_Q2 %>%
  group_by(addr_state) %>%
  summarise(total_loans = n(), .groups = "drop")

charged_off_loans_after <- after_2015_Q2 %>%
  filter(loan_status == "Charged Off") %>%
  group_by(addr_state) %>%
  summarise(charged_off_loans = n(), .groups = "drop")

# Step 6: Merge totals and charged off loans for both periods
frequency_data_before <- total_loans_before %>%
  left_join(charged_off_loans_before, by = "addr_state") %>%
  mutate(charged_off_loans = ifelse(is.na(charged_off_loans), 0, charged_off_loans),
         percentage_charged_off = (charged_off_loans / total_loans) * 100)

frequency_data_after <- total_loans_after %>%
  left_join(charged_off_loans_after, by = "addr_state") %>%
  mutate(charged_off_loans = ifelse(is.na(charged_off_loans), 0, charged_off_loans),
         percentage_charged_off = (charged_off_loans / total_loans) * 100)

# Step 7: Merge both frequency data with state coordinates
frequency_data <- state_coords %>%
  left_join(frequency_data_before %>% select(addr_state, percentage_charged_off) %>% rename(percentage_charged_off_before = percentage_charged_off), 
            by = c("state" = "addr_state")) %>%
  left_join(frequency_data_after %>% select(addr_state, percentage_charged_off) %>% rename(percentage_charged_off_after = percentage_charged_off), 
            by = c("state" = "addr_state"))

# Verify frequency data
print(frequency_data)

# Step 8: Filter to keep only points within the contiguous US
frequency_data <- frequency_data %>%
  filter(latitude >= 24.396308 & latitude <= 49.384358 & 
         longitude >= -125.0 & longitude <= -66.93457)

# Step 9: Create the map
us_map <- map_data("state")

# Step 10: Plot
# Step 10: Plot
ggplot() +
  geom_polygon(data = us_map, aes(x = long, y = lat, group = group), fill = "lightgrey") +
  geom_point(data = frequency_data, aes(x = longitude, y = latitude, size = percentage_charged_off_before), color = "red", alpha = 0.5) +
  geom_point(data = frequency_data, aes(x = longitude, y = latitude, size = percentage_charged_off_after), color = "green", alpha = 0.5) +
  scale_size_continuous(
    breaks = seq(0, 50, 0.1),
    range = c(5, 25),  # Adjust the range of sizes
    name = "Percentage of Charged Off Loans",
    guide = "none"
  ) +
  labs(title = "Percentage of Charged Off Loans by State (Before and After Q2 2015)",
       x = "Longitude",
       y = "Latitude") +
  theme_minimal()



```

```{r}
# Filter the data for only the relevant loan statuses and home ownership categories
df_filtered <- df %>%
  filter(loan_status %in% c("Charged Off", "Fully Paid", "Current") &
         home_ownership %in% c("MORTGAGE", "OWN", "RENT"))

# Create a contingency table
contingency_table <- table(df_filtered$loan_status, df_filtered$home_ownership)

# Remove rows and columns with only zero counts
contingency_table_filtered <- contingency_table[
  rowSums(contingency_table) > 0, 
  colSums(contingency_table) > 0
]

# View the filtered contingency table
print(contingency_table_filtered)

# Perform chi-squared test on the filtered table
chi_sq_result <- chisq.test(contingency_table_filtered)

# View the results of the chi-squared test
print(chi_sq_result)

expected_counts <- chisq.test(contingency_table_filtered)$expected
print(expected_counts)




```
---
title: "Chi-Square Test for Home Ownership and Loan Status"
author: "Your Name"
date: "`r Sys.Date()`"
output: html_document
---

## Hypotheses

- **Null Hypothesis (H₀):** Home ownership status does not affect loan status.
- **Alternative Hypothesis (H₁):** Home ownership status does affect loan status.

## Observed vs. Expected Counts

### Observed counts (from contingency table):

- **MORTGAGE:**
  - Charged Off: 107,903
  - Current: 429,307
  - Fully Paid: 511,353

- **OWN:**
  - Charged Off: 28,636
  - Current: 103,842
  - Fully Paid: 108,460

- **RENT:**
  - Charged Off: 116,600
  - Current: 344,481
  - Fully Paid: 372,044

### Expected counts (based on the chi-squared test):

- **MORTGAGE:**
  - Charged Off: 125,049
  - Current: 433,543.3
  - Fully Paid: 489,970.7

- **OWN:**
  - Charged Off: 28,733.66
  - Current: 99,619.25
  - Fully Paid: 112,585.09

- **RENT:**
  - Charged Off: 99,356.38
  - Current: 344,467.42
  - Fully Paid: 389,301.21

## Chi-Square Test Results

- **Chi-Square Statistic (X-squared):** 7413.6
- **Degrees of freedom (df):** 4
- **p-value:** < 2.2e-16

## Conclusion

Since the **p-value** is extremely small (**< 2.2e-16**), much smaller than the typical significance level of 0.05, we **reject the null hypothesis (H₀)**.

## Result

There is a **significant association** between **home ownership status** and **loan status**. This means that home ownership affects the likelihood of whether a loan is charged off, fully paid, or current.

This insight could be valuable for further analysis or decision-making based on how different home ownership categories influence loan outcomes.


























