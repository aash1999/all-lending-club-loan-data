---
title: "Analysis on LendingClub's Loan Data"
author: "Uyemaa Gantulga, Ayush Meshram, Aakash Singh, and Melissa Yago"
date: "10/20/24"
output:
  html_document:
    code_folding: show
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=F}
# Some of common RMD options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
knitr::opts_chunk$set(results="markup", warning = F, message = F)
# Can globally set option for number display format.
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
library(dplyr)
library(ezids)
library(ggplot2)
library(data.table)
library(fmsb)
library(corrplot)
library(lubridate)
library(maps)
library(ggmap)
source("./HelperFunctions/fetchSubset.R")
```

# Overview

Since 2006, LendingClub has been a U.S. financial services company that has facilitated loan contracts. Our research will focus on LendingClub's loan data that consist of approved and rejected loans between 2013-2018. To understand LendingClubâ€™s clientele, we will analyze borrower and loan characteristics to assess risk levels, determine loan acceptance, and understand what factors influence whether accepted loans will be fully repaid. To accomplish this, we will ask the following SMART questions:


1. How do the loan features of term, amount, and interest rate individually influence the likelihood of a loan being charged off (lender has accepted the loan as unlikely to recover) versus being fully paid? 

2. How do variables such as dti, annual_inc, int_rate, and fico_range impact the likelihood of a loan being "fully paid" or "charged off" year over year from 2014 to 2016? What trends can be identified in the data, and how do these variables differ between the two loan statuses over time?

3. Do the loan grades provided to each customer correlate to their loan repayment behavior based on income and loan status? 

4. Is there a relationship between employment length and the amount requested for rejected loans and how does it compare to the customers that had accepted loans?

# Data Dictionary

1. Funded Amount: The total amount of the loan to be paid off.

2. Loan Status: Whether the loan has been fully paid or has been charged off. A loan that is fully paid is defined as a loan where the total payment has been met. A charge off status is defined as the lender accepting the loan as uncollectible. This differs from the default status in which payments are late but the lender expects to collect what the borrower still owes.  

3. Term: How long the borrower has to pay off the loan (Either 36 or 60 Months).

4. Interest Rate: The percentage of the loan amount that the lender charges the borrower for borrowing money. 

5. DTI (Debt to Income Ratio): A measure used to evaluate a borrower's financial situation by comparing their debt payments to their gross monthly income. 

6. Annual Income: The dollar amount the customer earns on a yearly basis. 

7. FICO Range: A measure of creditworthiness used by lenders to assess a borrower's ability to repay a loan. 

8. Loan Grades: Classifies the risk level of a loan from A (less risky) to G (risky). 

9. Employment Length: The amount in years the customer has been employed. 


# Question 1: How do the loan features of term, amount, and interest rate individually influence the likelihood of a loan being charged off versus being fully paid? 

This question focuses on assessing risk levels and looking into trends of when the company can expect to see a loan charged off (lender has accepted the loan as unlikely to recover) or being paid off. 

Assumptions:
In terms of loan status, we are only analyzing the "Fully Paid" and "Charge Off" status.

## Data Types & Data Cleaning

For this question, we will be looking at funded amount, loan status, term, and interest rate. 

Based on this data, the funded amount and interest rate are data types that are numerical. On the other hand, loan status and term have been converted to factors and are categorical data types. While loan status logically makes sense to be categorical, term has been converted to a factor since there were only two types of loan terms (36 or 60 months) which can be seen as short term vs. long term when comparing the likelihood of default across loans. Term in this case will be viewed as nonlinear with interest rates and funded amount affecting risk with linearity.

To retrieve this information, data cleaning was done on the accepted loans dataset that initially contained 2,157,894 million observations. After filtering, the total observations stand at 1,245,285 observations.    

```{r}
# imports 



```

```{r, echo=True}
df <- read.csv("../DataSet/cleaned_accepted_2013_to_2018Q4.csv")

```

```{r, echo=TRUE}

#Subset for Specific Columns
f_df<- df[, c("funded_amnt", "loan_status","term","int_rate")]

#Filter for specific loan status
df_filter <- f_df %>%
  filter(loan_status %in% c("Charged Off", "Fully Paid"))

rm(f_df)

#Convert loan status and term as factors
df_filter$loan_status <- as.factor(df_filter$loan_status)
df_filter$term <- as.factor(df_filter$term)


#Select Relevant Columns
df_final <-df_filter %>%
  select(funded_amnt, term, int_rate, loan_status)

rm(df_filter)
#Check the Dataframe output
str(df_final)



```

## Normality of Loan Amount & Interest Rate

### QQ Plot of Loan Amount

```{r, echo=TRUE}

amount_plot = outlierKD2(df_final, funded_amnt, rm = TRUE, qqplt = TRUE, histogram = FALSE)
rm(amount_plot)

```

The loan amount data does not appear to follow a normal distribution based on the QQ plot, as there are significant deviations from the reference line, particularly at the tails. The lower quantiles fall below the line, while the upper quantiles rise far above it, indicating the presence of heavy tails. Additionally, the stepped line in the middle suggests that the data isn't evenly distributed. Even with the outliers removed, there are still extreme deviations at both ends therefore showing that this is not a normal distribution.    

### QQ Plot of Interest Rate


```{r, echo=TRUE}

rate_plot = outlierKD2(df_final, int_rate, rm = TRUE, qqplt = TRUE, histogram = FALSE)
rm(rate_plot)

```

The interest rate also shows similar trends. There are major deviations from the reference line, especially in the upper tail, indicating that the interest rate distribution is not normal when outliers are present. Even with the outliers removed, the QQ plot appears much more linear showing significant improvement towards normality, though there are deviations that remain in the tails. 

Overall, both loan amount and interest rate distributions are not normal when outliers are included. While showing improvement in normality when removing outliers, deviations still occur at the extremes. As a result, these datasets don't follow normality due to these deviations. 

## Descriptive Statistics

Initial statistics summarizes data that can help in identifying patterns, trends, and outliers. To demonstrate this, loan status will be analyzed against each loan feature (term, amount and interest rate) to see any relationships and patterns.  

### Relationship between Loan Status and Loan Term

```{r, echo=TRUE}


# Contingency Table (Correlation Analysis)
c_table <- table(df_final$term, df_final$loan_status)
cat("Contingency Table: Loan Status by Term\n\n")
print(c_table)

#Contingency Table by Proportions
prop_table <-prop.table(c_table) *100
cat("Contingency Table: Loan Status by Term Proportions (by %)\n\n")
print(prop_table)
rm(c_table)
# Stacked Bar Chart
ggplot(df_final, aes(x = term, fill = loan_status)) + 
  geom_bar(position = "stack") +
  labs(x="Loan Term", y="Number of Loans",title="Stacked Bar Chart of Loan Status by Term" ) +
  scale_fill_manual(values = c("navy", "lightblue"))

  
```

Via the first contingency table, 36 month loans have a higher absolute number of both fully paid and charged off loans compared to 60 month loans. Proportionally, 36 month loans have a higher rate of being charged off (12.32%) than 60 month loans (8.01%).

When visually shown on a stacked bar chart, you'll notice that shorter terms are more likelier to get charged off than loans with a longer term.  

Overall, shorter loan terms are likelier to result in a charge off and appear to influence loan status. 


### Relationship between Loan Status and Loan Amount

```{r, echo=TRUE}
# Descriptive Stats of Loan Amount Separated by Loan Status

amnt_stats <- df_final %>%
  group_by(loan_status) %>%
  summarise(
    Mean = mean(funded_amnt, na.rm = TRUE),
    Median = median(funded_amnt, na.rm = TRUE),
    SD = sd(funded_amnt, na.rm = TRUE),
    Q25 = quantile(funded_amnt, 0.25, na.rm = TRUE),
    Q50 = quantile(funded_amnt, 0.50, na.rm = TRUE),
    Q75 = quantile(funded_amnt, 0.75, na.rm = TRUE),
    Min = min(funded_amnt, na.rm = TRUE),
    Max = max(funded_amnt, na.rm = TRUE)
  )

cat("Descriptive Stats of Loan Amounts Separated by Loan Status)\n\n")
print(amnt_stats)
rm(amnt_stats)

```

```{r, echo=TRUE}
# Box Plot Comparing Loan Amounts by Loan Status
ggplot(df_final, aes(x = loan_status, y=funded_amnt, fill = loan_status)) + 
  geom_boxplot() + 
  geom_boxplot( colour="black", outlier.colour="blue", outlier.shape=8, outlier.size=4) +
  labs(title="Boxplot of Loan Amounts by Loan Status", x="Loan Status", y = "Loan Amount ($)")+ 
  scale_fill_manual(values = c("navy", "lightblue"))

```

Based on the statistics, the charged off loans having a higher mean and median loan amount, this can suggest that larger loan amounts may be associated with a higher risk of being charged off. Charged off loans also have a slightly higher standard deviation showing for more variability in the amounts.Lastly, both loan statuses show the same minimum and maximum but that does not necessarily mean that the distributions are identical.

Based on the boxplot, one commonality is the several outliers that exist around $40,000 for both loan statuses. However, this is not indicative that it is caused by status and requires further investigation through these high amounts being affected by other factors. Given the IQR of both statuses, there is not significant variability. You can however tell based on the IQR that the charged off loans have a slightly higher loan amounts compared to the fully paid loans that have a slightly lower loan amount.

Overall, larger loan amounts may be associated with a higher charged off risk. However holistically, loan amount alone does not appear to have a strong influence on whether a loan will be charged off or fully paid, given overlapping distributions. 


### Relationship Between Loan Status and Interest Rate

```{r, echo=TRUE}
# Descriptive Stats of Interest Rate Separated by Loan Status

rate_stats <- df_final %>%
  group_by(loan_status) %>%
  summarise(
    Mean = mean(int_rate, na.rm = TRUE),
    Median = median(int_rate, na.rm = TRUE),
    SD = sd(int_rate, na.rm = TRUE),
    Q25 = quantile(int_rate, 0.25, na.rm = TRUE),
    Q50 = quantile(int_rate, 0.50, na.rm = TRUE),
    Q75 = quantile(int_rate, 0.75, na.rm = TRUE),
    Min = min(int_rate, na.rm = TRUE),
    Max = max(int_rate, na.rm = TRUE)
  )

cat("Descriptive Stats of Interest Rates Separated by Loan Status)\n\n")
print(rate_stats)
rm(rate_stats)

```

```{r, echo=TRUE}

df_count <-df_final %>%
  filter(int_rate >0) %>%
  mutate(int_rate_bin = floor(int_rate/2)*2) %>%
  filter(int_rate_bin >0) %>%
  count(int_rate_bin, loan_status)

ggplot(df_count, aes(x = int_rate_bin, y = n, color = loan_status)) + 
  geom_line(size = 1)+ 
  labs(x="Interest Rate (%)", y="Count", title = "Distribution of Interest Rates by Loan Status") +
  scale_color_manual(values = c("navy", "lightblue")) 

rm(df_count)
rm(df_final)
```

Based on the statistics, higher interest rates significantly increase the likelihood of a loan being charged off. Charged off loans have a mean interest rate of 15.8%, compared to 12.6% for fully paid loans, with a similar difference in the median (15.1% vs. 12.1%). Even at the lower end of the interest rate range, loans that default tend to have higher rates. As charged off loans have a higher standard deviation and higher 75th percentile, this suggests greater volatility and risk. This indicates that interest rates are a strong predictor of loan defaults.  

Similar to the statistics, the histogram shows interest rates having a strong influence on loan outcomes. Loans with a higher interest rate of around 15% and above are more likely to be charged off (in proportion to fully paid), indicating a greater risk of default at these levels. In contrast, lower interest rates between 10% to 12% are associated with a higher frequency of fully paid loans, suggesting that borrowers with lower rates are more likely to repay their loans in full. There is also a noticeable overlap in the 10% to 15% interest rate range, where fully paid and charged off loans occur. But as interest rates increase, the likelihood of a loan being charged off rises significantly. This suggests that interest rates are a key predictor of loan performance, with higher rates correlating strongly with loan defaults and lower rates with successful repayment.  

Overall, loans with higher interest rates are likelier to be charged off, making interest rates a strong influence of loan charge offs. 



#Question 2 and answers go here:


```{r}
columns_to_target = c("annual_inc", "dti", "fico_range_high", "fico_range_low", "loan_amnt", 
  "int_rate", "earliest_cr_line", "revol_util", "delinq_2yrs", "pub_rec", 
  "total_acc", "open_acc", "installment", "home_ownership", 
  "verification_status", "delinq_amnt", "collections_12_mths_ex_med", 
  "chargeoff_within_12_mths", "mths_since_last_delinq", "purpose", "sub_grade", "issue_d", "addr_state","inq_last_12m","loan_status","chargeoff_within_12_mths")


#df <- fetch_subset(df_location = df_file_path, col_names = columns_to_target, nrows = c(1,-1))
df_final <- df %>% select(all_of(columns_to_target))

```




```{r}
convert_to_factors <- function(df) {
  # Columns to convert to factors based on the provided list
  factor_cols <- c("home_ownership", "verification_status", 
                   "loan_status", "purpose", "addr_state","sub_grade")
  
  df[] <- lapply(names(df), function(col) {
    if (col %in% factor_cols) {
      # Check if the column contains numeric-like values as strings and standardize them
      if (col == "chargeoff_within_12_mths") {
        # Convert '0.0' and '0' to '0', and handle similar cases
        df[[col]] <- as.character(df[[col]])
        df[[col]][df[[col]] == "0.0"] <- "0"
        df[[col]][df[[col]] == "1.0"] <- "1"  # If applicable, handle '1.0' similar to '1'
        return(as.factor(df[[col]]))  # Convert the cleaned column to factor
      } else {
        return(as.factor(df[[col]]))  # Convert other factor columns directly
      }
    } else if (is.character(df[[col]]) && all(!is.na(as.numeric(df[[col]][-1])))) {
      return(as.numeric(df[[col]]))  # Convert numeric-like strings to numeric
    } else {
      return(df[[col]])  # Leave other columns unchanged
    }
  })
  
  return(df)
}
df_final <- df_final[-1,]
df_final <- convert_to_factors(df_final)
#str(df_final)
```


```{r}
# Convert specified columns to numeric

df_final$revol_util <- as.numeric(df_final$revol_util)
df_final$mths_since_last_delinq <- as.numeric(df_final$mths_since_last_delinq)
df_final$dti <- as.numeric(df_final$dti)
df_final$inq_last_12m <- as.numeric(df_final$inq_last_12m)
df_final$chargeoff_within_12_mths <- as.numeric(df_final$chargeoff_within_12_mths)

# Optional: Check for any warnings about NAs being introduced during coercion
if (any(is.na(df_final$revol_util))) {
  warning("NAs introduced in 'revol_util' during conversion.")
}
if (any(is.na(df_final$mths_since_last_delinq))) {
  warning("NAs introduced in 'mths_since_last_delinq' during conversion.")
}
if (any(is.na(df_final$dti))) {
  warning("NAs introduced in 'dti' during conversion.")
}

```

```{r}
df_final$issue_d <- as.Date(df_final$issue_d, format = "%Y-%m-%d")
str(df_final)
```

```{r}
# LINE PLOT

# Create a copy of the dataframe
df1 <- df_final

# Step 1: Ensure 'issue_d' is in Date format
df1$issue_d <- as.Date(df1$issue_d, format = "%Y-%m-%d")  # Adjust the format if necessary

# Step 2: Extract year and quarter from 'issue_d'
df1 <- df1 %>%
  mutate(year = year(issue_d), 
         quarter = quarter(issue_d))

# Step 3: Filter out rows with NA values in year or quarter
df1 <- df1 %>%
  filter(!is.na(year), !is.na(quarter))

# Step 4: Calculate the total number of loans per quarter
df_total_per_quarter <- df1 %>%
  group_by(year, quarter) %>%
  summarise(total_loans = n(), .groups = 'drop')  # Adding .groups = 'drop' to avoid warnings

# Step 5: Filter for 'Charged Off' loan status
df_charged_off <- df1 %>%
  filter(loan_status == "Charged Off")

# Step 6: Aggregate 'Charged Off' counts by year and quarter
df_charged_off_aggregated <- df_charged_off %>%
  group_by(year, quarter) %>%
  summarise(charged_off_count = n(), .groups = 'drop')  # Adding .groups = 'drop' to avoid warnings

# Step 7: Merge the total loan counts with the 'Charged Off' counts
df_merged <- df_charged_off_aggregated %>%
  left_join(df_total_per_quarter, by = c("year", "quarter")) %>%
  mutate(percentage = (charged_off_count / total_loans) * 100)

# Step 8: Create a Year-Quarter variable in the correct format and order
df_merged <- df_merged %>%
  mutate(Year_Quarter = paste0(year, ".", quarter)) %>%
  arrange(year, quarter)  # Arrange the data to ensure it's in the correct order

# Step 9: Convert Year_Quarter into a factor with levels in the correct chronological order
df_merged$Year_Quarter <- factor(df_merged$Year_Quarter, 
                                 levels = df_merged$Year_Quarter)

# Step 10: Create the line plot with percentage and display percentage as text on each point
ggplot(df_merged, aes(x = Year_Quarter, y = percentage, group = 1)) +
  geom_area(fill = "red", alpha = 0.3) +  # Fill the area under the line with color and adjust opacity
  geom_line(color = "red", size = 1) +  # Line color and size
  geom_point(color = "red", size = 2) +  # Point color and size
  geom_text(aes(label = sprintf("%.1f%%", percentage)), 
            vjust = -0.5, size = 3, nudge_y = 0.5) +  # Display percentage above points
  labs(x = "Year-Quarter", y = "Charged Off Percentage", title = "Percentage of Charged Off Loans by Year and Quarter") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels
        panel.grid.major.y = element_blank(),  # Remove horizontal grid lines
        panel.grid.minor.y = element_blank())  # Remove minor horizontal grid lines


rm(df_charged_off)
rm(df1)
rm(df_charged_off_aggregated)
rm(df_merged)
rm(df_total_per_quarter)

```

```{r}
# T-test

# Ensure you're using the tidyverse for complete functionality
# Uncomment the following line if you want to load the entire tidyverse
# library(tidyverse)

# Step 1: Verify and convert 'issue_d' to Date if necessary
df_final$issue_d <- as.Date(df_final$issue_d, format = "%Y-%m-%d")  # Adjust the format if necessary

# Step 2: Filter the dataframe for 'Charged Off' loan_status and remove NaN values
charged_off_df <- df_final %>%
  filter(loan_status == "Charged Off") %>%
  filter(!is.na(issue_d))  # Ensure we have dates to work with

# Step 3: Create a new column for quarters if not already created
charged_off_df <- charged_off_df %>%
  mutate(quarter = quarter(issue_d), 
         year = year(issue_d),
         quarter_year = as.integer(format(issue_d, "%Y")) * 10 + ceiling(as.integer(format(issue_d, "%m")) / 3))

# Step 4: Check unique quarters available in the dataset
unique_quarters <- unique(charged_off_df$quarter_year)
#cat("Unique quarters in the dataset:", unique_quarters, "\n")

# Step 5: Split the data into two groups: before and including Q2 2015, and after Q2 2015
before_2015_Q2 <- charged_off_df %>%
  filter(year < 2015 | (year == 2015 & quarter <= 2))

after_2015_Q2 <- charged_off_df %>%
  filter(year > 2015 | (year == 2015 & quarter > 2))



# Proceed only if there are records in both groups
if (nrow(before_2015_Q2) > 0 && nrow(after_2015_Q2) > 0) {
  # Step 6: Identify numerical features in the filtered dataframe
  numerical_cols <- before_2015_Q2 %>%
    select_if(is.numeric) %>%
    colnames()

  # Step 7: Perform T-test for each numeric column, excluding those with zero observations in either group
  alpha <- 0.05  # Significance level
  t_test_results <- lapply(numerical_cols, function(col) {
    # Extract the two groups
    group1 <- before_2015_Q2[[col]]
    group2 <- after_2015_Q2[[col]]

    # Check if both groups have enough non-NA observations
    if (length(na.omit(group1)) > 0 && length(na.omit(group2)) > 0) {
      # Perform the t-test
      t_test_result <- t.test(group1, group2, var.equal = FALSE)  # Welch's t-test

      # Determine hypothesis status
      hypothesis_status <- ifelse(t_test_result$p.value <= alpha, "Reject H0", "Fail to Reject H0")

      return(data.frame(Feature = col, T_value = t_test_result$statistic, P_value = t_test_result$p.value, Hypothesis_Status = hypothesis_status))
    } else {
      return(NULL)  # Return NULL for insufficient data
    }
  })

  # Step 8: Remove NULL results and combine results into a dataframe
  t_test_results_df <- do.call(rbind, Filter(Negate(is.null), t_test_results))
  
  # Step 9: Remove unwanted features
  t_test_results_filtered <- t_test_results_df %>%
    filter(!Feature %in% c("quarter", "year", "quarter_year"))
  
  # Step 10: Print the filtered results
  print(t_test_results_filtered)

} else {
  cat("Not enough records in one of the groups to perform t-tests.\n")
}
rm(after_2015_Q2)
rm(before_2015_Q2)
rm(charged_off_df)
rm(t_test_results)
rm(t_test_results_df)
rm(t_test_results_filtered)


```


```{r}
#COOR PLOT

# Load necessary libraries

setDT(df_final)  # Converts df to a data.table, if it isn't one already

# Select only numeric columns from the data table
numeric_df <- df_final[, .SD, .SDcols = sapply(df_final, is.numeric)]

# Calculate the correlation matrix
cor_matrix <- cor(numeric_df, use = "pairwise.complete.obs")

# Set plotting parameters for larger plot size
# Create a larger plot window
options(repr.plot.width = 35, repr.plot.height = 45)  # Use this in RMarkdown or Jupyter Notebooks

# Plot the correlation matrix using corrplot without the title
corrplot(cor_matrix,
         type = "upper", 
         method = "circle", 
         addCoef.col = 0.2,  # Color for coefficients
         number.cex = 0.1,  # Size of the correlation coefficients
         tl.cex = 0.6,      # Size of the text labels for variables
         tl.col = "black",  # Color of the text labels
         cl.cex = 0.5)      # Size of the color legend text

# Remove numeric_df
rm(numeric_df)
rm(cor_matrix)

```


```{r}
# US MAP

# Load the state coordinates
state_coords <- read.csv("../DataSet/states.csv")

# Step 1: Verify and convert 'issue_d' to Date if necessary
df_final$issue_d <- as.Date(df_final$issue_d, format = "%Y-%m-%d")  # Adjust the format if necessary

# Step 2: Filter the dataframe and remove NaN values
charged_off_df <- df_final %>%
  filter(!is.na(issue_d), !is.na(addr_state))  # Ensure we have dates and states to work with

# Step 3: Create a new column for quarters
charged_off_df <- charged_off_df %>%
  mutate(quarter_year = as.integer(format(issue_d, "%Y")) * 10 + ceiling(as.integer(format(issue_d, "%m")) / 3))

# Step 4: Split the data into two groups: before and including Q2 2015, and after Q2 2015
before_2015_Q2 <- charged_off_df %>%
  filter(quarter_year <= 20152)

after_2015_Q2 <- charged_off_df %>%
  filter(quarter_year > 20152)

# Step 5: Calculate total loans and charged off loans by state for both periods
total_loans_before <- before_2015_Q2 %>%
  group_by(addr_state) %>%
  summarise(total_loans = n(), .groups = "drop")

charged_off_loans_before <- before_2015_Q2 %>%
  filter(loan_status == "Charged Off") %>%
  group_by(addr_state) %>%
  summarise(charged_off_loans = n(), .groups = "drop")

total_loans_after <- after_2015_Q2 %>%
  group_by(addr_state) %>%
  summarise(total_loans = n(), .groups = "drop")

charged_off_loans_after <- after_2015_Q2 %>%
  filter(loan_status == "Charged Off") %>%
  group_by(addr_state) %>%
  summarise(charged_off_loans = n(), .groups = "drop")

# Step 6: Merge totals and charged off loans for both periods
frequency_data_before <- total_loans_before %>%
  left_join(charged_off_loans_before, by = "addr_state") %>%
  mutate(charged_off_loans = ifelse(is.na(charged_off_loans), 0, charged_off_loans),
         percentage_charged_off = (charged_off_loans / total_loans) * 100)

frequency_data_after <- total_loans_after %>%
  left_join(charged_off_loans_after, by = "addr_state") %>%
  mutate(charged_off_loans = ifelse(is.na(charged_off_loans), 0, charged_off_loans),
         percentage_charged_off = (charged_off_loans / total_loans) * 100)

# Step 7: Merge both frequency data with state coordinates
frequency_data <- state_coords %>%
  left_join(frequency_data_before %>% select(addr_state, percentage_charged_off) %>% rename(percentage_charged_off_before = percentage_charged_off), 
            by = c("state" = "addr_state")) %>%
  left_join(frequency_data_after %>% select(addr_state, percentage_charged_off) %>% rename(percentage_charged_off_after = percentage_charged_off), 
            by = c("state" = "addr_state"))



# Step 8: Filter to keep only points within the contiguous US
frequency_data <- frequency_data %>%
  filter(latitude >= 24.396308 & latitude <= 49.384358 & 
         longitude >= -125.0 & longitude <= -66.93457)

# Step 9: Create the map
us_map <- map_data("state")


# Step 10: Plot with state symbols
ggplot() +
  # Base map
  geom_polygon(data = us_map, aes(x = long, y = lat, group = group), fill = "lightgrey") +
  
  # Points for charged off loans before Q2 2015
  geom_point(data = frequency_data, 
             aes(x = longitude, y = latitude, 
                 size = percentage_charged_off_before, 
                 color = "Before Q2 2015"), 
             alpha = 0.5) +
  
  # Points for charged off loans after Q2 2015
  geom_point(data = frequency_data, 
             aes(x = longitude, y = latitude, 
                 size = percentage_charged_off_after, 
                 color = "After Q2 2015"), 
             alpha = 0.5) +
  
  # Add state abbreviations
  geom_text(data = frequency_data, aes(x = longitude, y = latitude, label = state), 
            color = "black", size = 3, vjust = -0.8) +  # Adjust text size and position
  
  # Adjust the size scale
  scale_size_continuous(
    range = c(3, 10),  # Adjusting the size range
    name = "Percentage of Charged Off Loans"
  ) +
  
  # Define color scale
  scale_color_manual(values = c("Before Q2 2015" = "red", "After Q2 2015" = "green"),
                     name = "Time Period") +
  
  # Labels and title
  labs(title = "Percentage of Charged Off Loans by State (Before and After Q2 2015)",
       x = "Longitude",
       y = "Latitude") +
  
  theme_minimal() +
  theme(legend.position = "top")  # Place legend at the top

rm(after_2015_Q2)
rm(before_2015_Q2)
rm(charged_off_df)
rm(charged_off_loans_after)
rm(charged_off_loans_before)
rm(us_map)
rm(frequency_data)
rm(frequency_data_after)
rm(state_coords)
rm(total_loans_after)
rm(total_loans_before)
rm(frequency_data_before)

```

```{r}

# BOX PLOT

df1 <- df_final
# Ensure issue_d is in Date format
df1$issue_d <- as.Date(df1$issue_d, format = "%b-%Y")

# Filter out rows where issue_d is NA
df1 <- df1 %>%
  filter(!is.na(issue_d))

# Extract year and quarter from issue_d
df1$quarter <- paste0("Q", quarter(df1$issue_d), " ", year(df1$issue_d))

# Filter the data for only "Charged Off" and "Fully Paid" loan statuses
df_filtered <- df1 %>%
  filter(loan_status %in% c("Charged Off", "Fully Paid"))

# Define a function to remove outliers based on IQR
remove_outliers <- function(data) {
  Q1 <- quantile(data$dti, 0.25, na.rm = TRUE)
  Q3 <- quantile(data$dti, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  data %>%
    filter(dti >= lower_bound & dti <= upper_bound)
}

# Remove outliers by applying the function to each quarter and loan_status group
df_no_outliers <- df_filtered %>%
  group_by(quarter, loan_status) %>%
  do(remove_outliers(.))

# Ensure proper order of quarters
df_no_outliers$quarter <- factor(df_no_outliers$quarter, 
                                 levels = unique(df_no_outliers$quarter[order(df_no_outliers$issue_d)]))

# Plot box plot for dti for each quarter with loan_status distinction (without outliers)
ggplot(df_no_outliers, aes(x = quarter, y = dti, fill = loan_status)) +
  geom_boxplot() +
  labs(title = "DTI Box Plot by Quarter for Charged Off and Fully Paid Loans (Outliers Removed)",
       x = "Quarter",
       y = "DTI",
       fill = "Loan Status") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability
rm(df_filtered)
rm(df1)
rm(df_no_outliers)
rm(expected_counts)
rm(chi_sq_result)


```



```{r}
# RADAR CHART

# Step 1: Filter DataFrames for loans issued in 2014 and 2017
df_2014 <- df_final[df_final$issue_d >= as.Date("2015-01-01") & df_final$issue_d <= as.Date("2015-12-31"), ]
df_2017 <- df_final[df_final$issue_d >= as.Date("2018-01-01") & df_final$issue_d <= as.Date("2018-12-31"), ]

# Step 2: Function to remove outliers using IQR
# Function to remove outliers using IQR method
remove_outliers <- function(df, columns) {
  for (col in columns) {
    if (col %in% names(df)) {
      Q1 <- quantile(df[[col]], 0.25, na.rm = TRUE)
      Q3 <- quantile(df[[col]], 0.75, na.rm = TRUE)
      IQR_value <- IQR(df[[col]], na.rm = TRUE)
      
      # Define lower and upper bounds
      lower_bound <- Q1 - 1.5 * IQR_value
      upper_bound <- Q3 + 1.5 * IQR_value
      
      # Filter out the outliers
      df <- df[df[[col]] >= lower_bound & df[[col]] <= upper_bound, ]
    } else {
      message(paste("Column", col, "not found in the dataframe. Skipping."))
    }
  }
  return(df)
}

# Step 3: Select the columns of interest for standardization and outlier removal
selected_columns <- c("int_rate", "annual_inc", "fico_range_low", "fico_range_high", "dti")
# Remove outliers in each group
df_2014 <- remove_outliers(df_2014, selected_columns)
df_2017 <- remove_outliers(df_2017, selected_columns)

# Step 4: Function to standardize numeric columns (scaling from 0 to 1)
# Convert df_2014 to a data.table
setDT(df_2014)
setDT(df_2017)

# Define the standardization function using data.table syntax
standardize_columns <- function(dt, selected_cols) {
  dt[, (selected_cols) := lapply(.SD, function(x) (x - min(x, na.rm = TRUE)) / 
                                               (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))), 
       .SDcols = selected_cols]
  return(dt)
}


# Standardize the selected columns in each group
df_2014 <- standardize_columns(df_2014, selected_columns)
df_2017 <- standardize_columns(df_2017, selected_columns)

# Step 5: Calculate the medians for selected numeric columns
medians_2014 <- df_2014[, lapply(.SD, median, na.rm = TRUE), .SDcols = selected_columns]
medians_2017 <- df_2017[, lapply(.SD, median, na.rm = TRUE), .SDcols = selected_columns]

# Step 6: Combine the medians into a data frame for radar chart
radar_data_2014_2017 <- rbind(as.data.frame(medians_2014), 
                               as.data.frame(medians_2017))

# Step 7: Assign appropriate row names for each category
rownames(radar_data_2014_2017) <- c("2014", "2017")

# Step 8: Add max and min rows for radar chart
radar_data_2014_2017 <- rbind(max = rep(1, ncol(radar_data_2014_2017)), 
                               min = rep(0, ncol(radar_data_2014_2017)), 
                               radar_data_2014_2017)

# Set the background color to white
par(bg = "white")  

# Step 9: Plot the radar chart for 2014 and 2017
radarchart(radar_data_2014_2017, 
           axistype = 1,  # Set axis type
           pcol = c("#1E90FF", "#32CD32"),  # Brighter line colors for 2014 and 2017
           plty = 1,  # Line type
           title = "Comparison of Loan Variables (2014 vs. 2017)",
           cglcol = "lightgrey",  # Lighter grid line color
           cglty = 1,  # Type of the grid lines
           caxislabels = seq(0, 1, 0.1),  # Customize axis labels (0 to 1 range)
           axislabcol = "black",  # Axis label color
           vlcex = 0.8,  # Text size of labels
           titlecol = "black",  # Title color
           cglwd = 0.8)  # Thickness of the grid lines

# Step 10: Add legend to the radar chart
legend("topright",  # Position of the legend
       legend = c("2014", "2017"),  # Labels for the legend
       col = c("#1E90FF", "#32CD32"),  # Corresponding bright colors
       lty = 1,  # Line type in the legend
       bty = "n",  # No box around the legend
       text.col = "black",  # Text color for the legend
       cex = 0.8)  # Text size of the legend
rm(df_2014)
rm(df_2017)
rm(medians_2014)
rm(medians_2017)
rm(radar_data_2014_2017)

```













```{r}
col_names = c("Debt-To-Income Ratio", "Risk_Score","Application Date")
dfr <- fetch_subset(df_location = "../DataSet/filtered_rejected_2013_to_2018Q.csv", col_names = col_names, nrows = c(1,-1))
set.seed(1)  # For reproducibility
dfr <- dfr %>% sample_frac(0.25)
```



```{r}
rm(df_final)
str(dfr)

```



```{r}
# Contour PLOT

# Clone the original dataframe
dfr1 <- dfr

# Convert Application Date to Date format if not already done
dfr1$Application_Date <- as.Date(dfr1$'Application Date')

# Convert Risk_Score to numeric, handling NAs
dfr1$Risk_Score <- as.numeric(dfr1$Risk_Score)

# Convert Debt-To-Income Ratio to numeric by removing "%" and converting to decimal
dfr1$Debt_To_Income_Ratio <- as.numeric(gsub("%", "", dfr1$`Debt-To-Income Ratio`)) / 100

# Filter for application dates before and after 2015
dfr_before_2015 <- dfr1 %>% filter(Application_Date < as.Date("2015-01-01"))
dfr_after_2015 <- dfr1 %>% filter(Application_Date >= as.Date("2015-01-01"))

# Define outlier removal function using IQR
remove_outliers <- function(data) {
  # Calculate Q1, Q3, and IQR for Risk_Score
  Q1_risk <- quantile(data$Risk_Score, 0.25, na.rm = TRUE)
  Q3_risk <- quantile(data$Risk_Score, 0.75, na.rm = TRUE)
  IQR_risk <- IQR(data$Risk_Score, na.rm = TRUE)
  
  # Calculate Q1, Q3, and IQR for Debt_To_Income_Ratio
  Q1_dti <- quantile(data$Debt_To_Income_Ratio, 0.25, na.rm = TRUE)
  Q3_dti <- quantile(data$Debt_To_Income_Ratio, 0.75, na.rm = TRUE)
  IQR_dti <- IQR(data$Debt_To_Income_Ratio, na.rm = TRUE)

  # Filter data to remove outliers
  data_filtered <- data %>%
    filter(
      Risk_Score >= (Q1_risk - 1.5 * IQR_risk) & Risk_Score <= (Q3_risk + 1.5 * IQR_risk) &
      Debt_To_Income_Ratio >= (Q1_dti - 1.5 * IQR_dti) & Debt_To_Income_Ratio <= (Q3_dti + 1.5 * IQR_dti)
    )
  
  return(data_filtered)
}

# Remove outliers from both datasets
dfr_before_2015_clean <- remove_outliers(dfr_before_2015)
dfr_after_2015_clean <- remove_outliers(dfr_after_2015)

# Combine the datasets for plotting
dfr_combined <- bind_rows(
  dfr_before_2015_clean %>% mutate(Period = "Before 2015"),
  dfr_after_2015_clean %>% mutate(Period = "After 2015")
)

# Create a combined contour plot
ggplot(dfr_combined, aes(x = Risk_Score, y = Debt_To_Income_Ratio, color = Period)) +
  geom_density_2d() +
  labs(title = "Contour Plot of Risk Score vs Debt-To-Income Ratio",
       x = "Risk Score",
       y = "Debt-To-Income Ratio") +
  scale_color_manual(values = c("Before 2015" = "blue", "After 2015" = "red")) +
  theme_minimal() +
  theme(legend.title = element_blank())

# Clean up
rm(dfr_before_2015)
rm(dfr_after_2015)
rm(dfr1)
rm(dfr_before_2015_clean)
rm(dfr_after_2015_clean)

```





```{r}
# HISTOGRAM 

# Load necessary libraries
library(ggplot2)
library(dplyr)

dfr1 <- dfr

# Ensure the Debt-To-Income Ratio is numeric
dfr1$dti <- as.numeric(gsub("%", "", dfr1$`Debt-To-Income Ratio`))  # Remove '%' and convert to numeric

# Ensure the Application Date is in Date format
dfr1$`Application Date` <- as.Date(dfr1$`Application Date`, format = "%Y-%m-%d")

# Remove NA values (if any) in Debt-To-Income Ratio and Application Date
dfr1 <- dfr1 %>% filter(!is.na(dti) & !is.na(`Application Date`))

# Define a function to remove outliers based on IQR
remove_outliers <- function(data) {
  Q1 <- quantile(data$dti, 0.25, na.rm = TRUE)  # 1st quartile (25th percentile)
  Q3 <- quantile(data$dti, 0.75, na.rm = TRUE)  # 3rd quartile (75th percentile)
  IQR <- Q3 - Q1  # Interquartile range
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  data %>%
    filter(dti >= lower_bound & dti <= upper_bound)  # Remove outliers
}

# Apply the function to remove outliers
dfr_no_outliers <- remove_outliers(dfr1)

# Split the data into before and after 2015
dfr_before_2015 <- dfr_no_outliers %>% filter(`Application Date` < as.Date("2015-01-01"))
dfr_after_2015 <- dfr_no_outliers %>% filter(`Application Date` >= as.Date("2015-01-01"))

# Create the frequency distribution plot using ggplot
ggplot() +
  geom_histogram(data = dfr_before_2015, aes(x = dti, y = ..count.., color = "Before 2015", fill = "Before 2015"), 
                 binwidth = 1, alpha = 0.4, position = "identity") +
  geom_histogram(data = dfr_after_2015, aes(x = dti, y = ..count.., color = "After 2015", fill = "After 2015"), 
                 binwidth = 1, alpha = 0.4, position = "identity") +
  labs(title = "Frequency Distribution of Debt-To-Income Ratio (Approved Loans)",
       x = "Debt-To-Income Ratio (%)",
       y = "Frequency",
       color = "Period",
       fill = "Period") +
  theme_minimal() +
  scale_color_manual(values = c("Before 2015" = "blue", "After 2015" = "red")) +
  scale_fill_manual(values = c("Before 2015" = "blue", "After 2015" = "red")) +
  theme(legend.position = "top")

# Optional: Clean up memory
rm(dfr1)
rm(dfr_no_outliers)
rm(dfr_before_2015)
rm(dfr_after_2015)
rm(df_no_outliers)


```



```{r}
# DENSITY GRAPH

# Load necessary libraries
library(ggplot2)
library(dplyr)

dfr1 <- dfr

# Ensure the Debt-To-Income Ratio is numeric
dfr1$dti <- as.numeric(gsub("%", "", dfr1$`Risk_Score`))  # Remove '%' and convert to numeric

# Ensure the Application Date is in Date format
dfr1$`Application Date` <- as.Date(dfr1$`Application Date`, format = "%Y-%m-%d")

# Remove NA values (if any) in Debt-To-Income Ratio and Application Date
dfr1 <- dfr1 %>% filter(!is.na(dti) & !is.na(`Application Date`))

# Define a function to remove outliers based on IQR
remove_outliers <- function(data) {
  Q1 <- quantile(data$dti, 0.25, na.rm = TRUE)  # 1st quartile (25th percentile)
  Q3 <- quantile(data$dti, 0.75, na.rm = TRUE)  # 3rd quartile (75th percentile)
  IQR <- Q3 - Q1  # Interquartile range
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  data %>%
    filter(dti >= lower_bound & dti <= upper_bound)  # Remove outliers
}

# Apply the function to remove outliers
dfr_no_outliers <- remove_outliers(dfr1)

# Split the data into before and after 2015
dfr_before_2015 <- dfr_no_outliers %>% filter(`Application Date` < as.Date("2015-01-01"))
dfr_after_2015 <- dfr_no_outliers %>% filter(`Application Date` >= as.Date("2015-01-01"))

# Create the line distribution plot using ggplot
ggplot() +
  geom_density(data = dfr_before_2015, aes(x = dti, color = "Before 2015"), size = 1.2) +
  geom_density(data = dfr_after_2015, aes(x = dti, color = "After 2015"), size = 1.2) +
  labs(title = "Density Plot of Risk Score (Before and After 2015)",
       x = "Risk Score",
       y = "Density",
       color = "Period") +
  theme_minimal() +
  scale_color_manual(values = c("Before 2015" = "blue", "After 2015" = "red"))

rm(dfr1)
rm(dfr_no_outliers)
rm(dfr_before_2015)
rm(dfr_after_2015)


```
```{r}
rm(dfr_combined)
rm(dfr1)
```




#Question 3 and answers go here:



#Question 4 and answers go here:


#Conclusion is here. 



